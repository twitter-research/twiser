{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd677a1",
   "metadata": {},
   "source": [
    "# TWISER Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37dd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from twiser import twiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefd03c",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "In this demo, we use the open access data from the study [The Generalizability of Survey Experiments](https://www.ipr.northwestern.edu/documents/working-papers/2014/IPR-WP-14-19.pdf).\n",
    "The data is found on [Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/MUJHGR).\n",
    "\n",
    "For reproducibility, the SHA1 hashes of the full data zip and csv are:\n",
    "```\n",
    "b1478ffac1906e5381ad3456b22f63d88c2ac6c9  dataverse_files.zip\n",
    "465255af3a6f7fc413f53a8d4d157f01f8c6002b  Study1Data.csv\n",
    "```\n",
    "Put the data in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0f09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./Study1Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b602efa2",
   "metadata": {},
   "source": [
    "## Select an experiment from the study to analyze\n",
    "\n",
    "The study considered the consistency of results when surveys are done using convenience samples from various sources. In this demo, we consider applying variance reduction to analyzing the results of a single question collected via a social media survey.\n",
    "\n",
    "The study asked a question in two different ways as the intervention and treatment effect was to measure how much effect there was on the response (measured on a 1 to 7 level of agreement).\n",
    "\n",
    "Here, the response is a 1 to 7 level of agreement with:\n",
    "\"Do you oppose or support the proposal to forgive student loan debt?\"\n",
    "The intervention was to ask the question as either:\n",
    "\n",
    "  - **Control**: \"According to the U.S. Department of Education, college student loan debt now exceeds one trillion dollars, which surpasses the total credit card debt in the United States. This has led to proposals for a student loan forgiveness program.\"\n",
    "  - **Treatment**: \"According to the U.S. Department of Education, college student loan debt now exceeds one trillion dollars, which surpasses the total credit card debt in the United States. This has led to proposals for a student loan forgiveness program. A number of expert economic analysts suggest that a student loan forgiveness program would have serious negative effects on the economy. When individuals accept a student loan, they know they are required to pay it back. By transferring this individual responsibility and debt to the national government, the burden falls on all taxpayers and lets students avoid their financial obligations.\"\n",
    "\n",
    "We use three pre-experiment individual features for variance reduction and improve statistical power: sex, party affiliation, and identified political ideology. Because these feature are determinied *before* the randomized assignment to control or treatment, they can be used for variance reduction. If these features are useful for predicting an individual's response then they will increase statistical power and shrink confidence intervals.\n",
    "\n",
    "For details on the columns and codings see the master codebook in the downloaded data zip:\n",
    "```\n",
    "Study1Codebook.docx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb59fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from the csv file to use\n",
    "pre_experiment_cols = [\"Sex\", \"Party\", \"Ideo\"]\n",
    "sample_col = \"Sample\"\n",
    "treatment_col = \"LoanGroup\"\n",
    "response_col = \"LoanSupp\"\n",
    "\n",
    "# Codes for sample groups to use from master codebook\n",
    "control_code = 1\n",
    "treatment_code = 2\n",
    "social_media_sample_code = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797f8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full list of columns we will be using\n",
    "cols_to_keep = pre_experiment_cols + [sample_col, treatment_col, response_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4b9a7",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here we load the data.\n",
    "It takes a bit of effort to correct for some of the messiness for this kind of data and encode everything as a float.\n",
    "We need to impute some of the pre-experiment features with missing values, but as long as the distribution on pre-experiment features remains unchanged between treatment and control it does not invalidate the variance reduction analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827bbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-standard missing values in csv\n",
    "na_values = [\"-1\", \"-99\", \".\", \"?\"]\n",
    "\n",
    "# Do the load and clean\n",
    "df = pd.read_csv(fname, header=0, index_col=0, na_values=na_values, low_memory=False)\n",
    "df = df[cols_to_keep]  # Only keep relvant columns\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\")  # Enforce everything to be float\n",
    "df[pre_experiment_cols] = df[pre_experiment_cols].fillna(df[pre_experiment_cols].mean())  # Impute missing features\n",
    "\n",
    "# Check that was all done correctly\n",
    "assert (df.dtypes == float).all()\n",
    "assert not df[pre_experiment_cols].isnull().any().any()\n",
    "# The current columns should all be in 1--7, this will also catch if any -1 or -99 missing values got through cleaning\n",
    "assert not (df < 1).any().any()\n",
    "assert not (df > 7).any().any()\n",
    "\n",
    "# And take the data via the social media survey\n",
    "df = df[df[sample_col] == social_media_sample_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c4d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out control and treatment\n",
    "is_control = df[treatment_col] == control_code\n",
    "is_treatment = df[treatment_col] == treatment_code\n",
    "assert not (is_control & is_treatment).any()\n",
    "\n",
    "# Extract the arrays\n",
    "x = df.loc[is_treatment, response_col].values\n",
    "y = df.loc[is_control, response_col].values\n",
    "x_covariates = df.loc[is_treatment, pre_experiment_cols].values\n",
    "y_covariates = df.loc[is_control, pre_experiment_cols].values\n",
    "\n",
    "# Ignore any rows where the response is missing\n",
    "x_covariates = x_covariates[~np.isnan(x), :]\n",
    "x = x[~np.isnan(x)]\n",
    "y_covariates = y_covariates[~np.isnan(y), :]\n",
    "y = y[~np.isnan(y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551f5c1",
   "metadata": {},
   "source": [
    "## Setup the predictor and use TWISER\n",
    "\n",
    "We want a nonlinear predictor that won't extrapolate for poor predictions, so RF is a good choice.\n",
    "We use MSE as the criterion since the variance reduction is proportional predictive MSE.\n",
    "For reproducibility, we also fix the RF random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fdcd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = RandomForestRegressor(criterion=\"squared_error\", random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8414c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output(estimate, ci, pval):\n",
    "    \"\"\"Helper function to print the results of the statical tests in TWISER.\"\"\"\n",
    "    (lb, ub) = ci\n",
    "    sig_mark = \"*\" if pval < 0.05 else \"\"\n",
    "    print(f\"estimate: {estimate:.2f} in ({lb:.2f}, {ub:.2f}), CI width of {ub - lb:.2f}, p = {pval:.4f}{sig_mark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5dc8c",
   "metadata": {},
   "source": [
    "### Basic $z$-test\n",
    "\n",
    "First, we apply the basic two-sample $z$-test included in twiser. This works basically the same as `scipy.stats.ttest_ind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dff334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 0.80 in (-0.14, 1.75), CI width of 1.89, p = 0.0954\n"
     ]
    }
   ],
   "source": [
    "estimate, (lb, ub), pval = twiser.ztest(x, y, alpha=0.05)\n",
    "show_output(estimate, (lb, ub), pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424236c",
   "metadata": {},
   "source": [
    "### Variance reduction with held out data\n",
    "\n",
    "Next, we apply variance reduction where the predictor was trained on a held out 30% of the data.\n",
    "This is the easiest to show validity, but some of the added power is lost because not all data is used in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e633d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 1.40 in (0.20, 2.59), CI width of 2.39, p = 0.0217*\n"
     ]
    }
   ],
   "source": [
    "estimate, (lb, ub), pval = twiser.ztest_cv_train(x, x_covariates, y, y_covariates, alpha=0.05, train_frac=0.3, predictor=predictor, random=np.random.RandomState(123))\n",
    "show_output(estimate, (lb, ub), pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70271350",
   "metadata": {},
   "source": [
    "### Variance reduction with cross validation\n",
    "\n",
    "To be more statistically efficient we train and predict using 10-fold cross validation.\n",
    "Here, no data is wasted.\n",
    "As we can see it is a more significant result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e18278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 1.38 in (0.51, 2.25), CI width of 1.74, p = 0.0019*\n"
     ]
    }
   ],
   "source": [
    "estimate, (lb, ub), pval = twiser.ztest_stacked_train(x, x_covariates, y, y_covariates, alpha=0.05, k_fold=10, predictor=predictor, random=np.random.RandomState(123))\n",
    "show_output(estimate, (lb, ub), pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86f7f3",
   "metadata": {},
   "source": [
    "### Variance reduction in-sample\n",
    "\n",
    "In the literature it is popular to train the predictor in the same sample as the test.\n",
    "This often gives the most power.\n",
    "However, any overfitting in the predictor can also invalidate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586d549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate: 0.86 in (0.24, 1.49), CI width of 1.24, p = 0.0065*\n"
     ]
    }
   ],
   "source": [
    "estimate, (lb, ub), pval = twiser.ztest_in_sample_train(x, x_covariates, y, y_covariates, alpha=0.05, predictor=predictor, random=np.random.RandomState(123))\n",
    "show_output(estimate, (lb, ub), pval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic_jn",
   "language": "python",
   "name": "generic_jn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
